# ðŸ§¬ Network Slimming Replication â€“ Channel-Level Model Compression

This repository provides a **PyTorch-based replication** of  
**Learning Efficient Convolutional Networks through Network Slimming**.

The focus is **translating the theoretical idea of channel slimming into a clean, practical implementation**,  
rather than achieving state-of-the-art accuracy or running full-scale training.

- Channel importance encoded by **BatchNorm scaling factors (Î³)** ðŸœ  
- Structured compression via **L1 regularization** on normalization layers ðŸœ‚  
- Simple and interpretable **channel pruning pipeline** ðŸœƒ  

**Paper reference:** [Network Slimming â€“ Liu et al., 2017](https://arxiv.org/abs/1708.06519) ðŸœ„

---

## ðŸŒŒ Overview â€“ Network Slimming Pipeline

![Network Slimming Overview](images/figmix.jpg)

The core idea:

> Learn channel importance during training â†’ remove unimportant channels â†’ fine-tune a compact network.

High-level procedure:

1. Start from an **over-parameterized CNN** with Batch Normalization layers.

2. Train the network using standard classification loss augmented with channel sparsity regularization.

3. Channels associated with small BatchNorm scaling factors are considered unimportant and removed.

4. Fine-tune the pruned model to recover performance.

This process can be applied once or iteratively to obtain progressively slimmer networks.

---

## ðŸ§® Channel Importance via Batch Normalization

Batch Normalization is defined as:

$$
\hat{z} = \frac{z_{in} - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}, \quad
z_{out} = \gamma \hat{z} + \beta
$$

Here, $\gamma$ is a **learnable scaling factor** associated with each channel.  
In Network Slimming, these scaling factors act as **implicit channel importance indicators**.

By applying **L1 regularization** to $\gamma$, unimportant channels are encouraged to shrink toward zero,  
making them natural candidates for pruning.

---

## ðŸ§  What the Model Learns

- **BatchNorm Î³ parameters** automatically encode channel usefulness  
- Channels with negligible contribution are suppressed during training  
- Pruning becomes a deterministic and structured operation  
- The resulting model is **smaller, faster, and easier to deploy**

---

## ðŸ“¦ Repository Structure

```bash
NetworkSlimming-Replication/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ conv_block.py          # Convolutional building blocks
â”‚   â”‚   â”œâ”€â”€ activation.py          # ReLU and activation helpers
â”‚   â”‚   â”œâ”€â”€ normalization.py       # BatchNorm wrappers
â”‚   â”‚   â””â”€â”€ pooling.py             # Pooling layers
â”‚   â”‚
â”‚   â”œâ”€â”€ backbone/
â”‚   â”‚   â””â”€â”€ cnn_blocks.py
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ slimming_cnn.py        # CNN with Network Slimming logic
â”‚   â”‚
â”‚   â”œâ”€â”€ pruning/
â”‚   â”‚   â””â”€â”€ prune_channels.py     # Channel pruning based on Î³ values
â”‚   â”‚
â”‚   â”œâ”€â”€ loss/
â”‚   â”‚   â””â”€â”€ cross_entropy_l1.py   # Cross-entropy + L1 regularization
â”‚   â”‚
â”‚   â””â”€â”€ config.py                 # Hyperparameters and pruning ratios
â”‚
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ figmix.jpg                # Network Slimming overview
â”‚   â””â”€â”€ math.jpg                  # BatchNorm formulation
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---


## ðŸ”— Feedback

For questions or feedback, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)
